The cross-entropy (CE) method is a probabilistic optimization approach that attempts to iteratively fit a distribution to elite samples from an initial proposal distribution \cite{rubinstein2004cross,rubinstein1999cross}.
The goal is to estimate a rare-event probability by minimizing the \textit{cross-entropy} between the two distributions \cite{de2005tutorial}.
The CE-method has gained popularity in part due to its simplicity in implementation and straightforward derivation.
The technique uses \textit{importance sampling} which introduces a proposal distribution over the rare-events to sample from then re-weights the posterior likelihood by the \textit{likelihood ratio} of the true distribution over the proposal distribution.

There are a few key assumptions that make the CE-method work effectively.
Through random sampling, the CE-method assumes that there are enough objective function evaluations to accurately represent the objective. 
This may not be a problem for simple applications, but can be an issue for computationally expensive objective functions. 
Another assumption is that the initial parameters of the proposal distribution are wide enough to cover the design space of interest. For the case with a multivariate Gaussian distribution, this corresponds to an appropriate mean and wide covariance.
In rare-event simulations with many local minima, the CE-method can fail to find the global minimum especially with sparse objective function evaluations.

This work aims to address the key assumptions of the CE-method.
We introduce variants of the CE-method that use surrogate modeling to approximate the objective function, thus updating the belief of the underlying objective through estimation.
As part of this approach, we introduce evaluation scheduling techniques to reallocate true objective function calls earlier in the optimization when we know the covariance will be large.
The evaluation schedules can be based on a distribution (e.g., the Geometric distribution) or can be prescribed manually depending on the problem.
We also use a Gaussian mixture model representation of the prior distribution as a method to explore competing local optima.
While the use of Gaussian mixture models in the CE-method is not novel, we connect the use of mixture models and surrogate modeling in the CE-method.
This connection uses each elite sample as the mean of a component distribution in the mixture, optimized through a subroutine call to the standard CE-method using the learned surrogate model.
To test our approach, we introduce a parameterized test objective function called \textit{sierra}.
The sierra function is built from a multivariate Gaussian mixture model with many local minima and a single global minimum.
Parameters for the sierra function allow control over both the spread and distinction of the minima.
Lastly, we provide an analysis of the weak areas of the CE-method compared to our proposed variants.


\section{Related Work} \label{sec:cem_related_work}
The cross-entropy method is popular in the fields of operations research, machine learning, and optimization \cite{kochenderfer2015decision,kochenderfer2019algorithms}.
The combination of the cross-entropy method, surrogate modeling, and mixture models has been explored in other work \cite{bardenet2010surrogating}. 
\citefull{bardenet2010surrogating} proposed an adaptive grid approach to accelerate Gaussian-process-based surrogate modeling using mixture models as the prior in the cross-entropy method. Unsurprisingly, they showed that a mixture model performs better than a single Gaussian when the objective function is multimodal.
Our work differs in that we augment the ``elite'' samples both by an approximate surrogate model and by a subroutine call to the CE-method using the learned surrogate model.
Other related work use Gaussian processes and a modified cross-entropy method for receding-horizon trajectory optimization \cite{tan2018gaussian}.
Their cross-entropy method variant also incorporates the notion of exploration in the context of path finding applications.
An approach based on \textit{relative entropy}, described in \cref{sec:cem_background_ce}, proposed a model-based stochastic search that seeks to minimize the relative entropy \cite{NIPS2015_5672}. They also explore the use of a simple quadratic surrogate model to approximate the objective function.
Prior work that relate cross-entropy-based adaptive importance sampling with Gaussian mixture models show that when using mixture models, fewer objective function calls are required relative to a na\"ive Monte Carlo or standard unimodal cross-entropy-based importance sampling method \cite{kurtz2013cross,wang2016cross}.


\section{Background} \label{sec:cem_background}
This section provides necessary background on techniques used in this work. We provide introductions to cross-entropy and the cross-entropy method, surrogate modeling using Gaussian processes, and multivariate Gaussian mixture models.

\subsection{Cross-Entropy} \label{sec:cem_background_ce}
Before understanding the cross-entropy method, we first must understand the notion of \textit{cross-entropy}.
Cross-entropy is a metric used to measure the distance between two probability distributions, where the distance may not be symmetric \cite{de2005tutorial}.
The distance used to define cross-entropy is called the \textit{Kullback-Leibler (KL) distance} or \textit{KL divergence}.
The KL distance is also called the \textit{relative entropy}, and we can use this to derive the cross-entropy.
Formally, for a random variable $\mat{X} = (X_1, \ldots, X_n)$ with a support of $\mathcal{X}$, the KL distance between two continuous probability density functions $f$ and $g$ is defined to be:
\begin{align*}
    \mathcal{D}_\text{KL}(f \mid\mid g) &= \E_f\left[\log \frac{f(\vec{X})}{g(\vec{X})} \right]\\
                      &= \int\limits_{\vec{x} \in \mathcal{X}} f(\vec{x}) \log f(\vec{x}) d\vec{x} - \int\limits_{\vec{x} \in \mathcal{X}} f(\vec{x}) \log g(\vec{x}) d\vec{x}
\end{align*}
We denote the expectation of some function with respect to a distribution $f$ as $\E_f$.
Minimizing the KL distance $\mathcal{D}_\text{KL}$ between our true distribution $f$ and our proposal distribution $g$ parameterized by $\vec{\theta}$, is equivalent to choosing $\vec\theta$ that minimizes the following, called the \textit{cross-entropy}:
\begin{align*}
    H(f,g) &= H(f) + \mathcal{D}_\text{KL}(f \mid\mid g)\\
           &= -\E_f[\log g(\vec{X})] \tag{using KL distance}\\
           &= - \int\limits_{\vec{x} \in \mathcal{X}} f(\vec{x}) \log g(\vec{x}; \vec{\theta}) d\vec{x}
\end{align*}
where $H(f)$ denotes the entropy of the distribution $f$ (where we conflate entropy and continuous entropy for convenience).
This assumes that $f$ and $g$ share the support $\mathcal{X}$ and are continuous with respect to $\vec{x}$.
The minimization problem then becomes:
\begin{equation} \label{eq:min}
\begin{aligned}
    \minimize_{\vec{\theta}} & & - \int\limits_{\vec{x} \in \mathcal{X}} f(\vec{x}) \log g(\vec{x}; \vec{\theta}) d\vec{x}
\end{aligned}
\end{equation}
Efficiently finding this minimum is the goal of the cross-entropy method algorithm.


\subsection{Cross-Entropy Method} \label{sec:cem_background_cem}
Using the definition of cross-entropy, intuitively the \textit{cross-entropy method} (CEM or CE-method) aims to minimize the cross-entropy between the unknown true distribution $f$ and a proposal distribution $g$ parameterized by $\vec\theta$.
This technique reformulates the minimization problem as a probability estimation problem, and uses adaptive importance sampling to estimate the unknown expectation \cite{de2005tutorial}.
The cross-entropy method has been applied in the context of both discrete and continuous optimization problems \cite{rubinstein1999cross,kroese2006cross}.


The initial goal is to estimate the probability 
\begin{align*}
    \ell = P_{\vec{\theta}}(S(\vec{x}) \ge \gamma)
\end{align*}
where $S$ can the thought of as an objective function of $\vec{x}$, and $\vec{x}$ follows a distribution defined by $g(\vec{x}; \vec{\theta})$.
We want to find events where our objective function $S$ is above some threshold $\gamma$.
We can express this unknown probability as the expectation
\begin{align} \label{eq:expect}
    \ell = \E_{\vec{\theta}}\Big[\mathbbm{1}\{S(\vec{x}) \ge \gamma\}\Big]
\end{align}
where $\mathbbm{1}$ denotes the indicator function.
A straightforward way to estimate \cref{eq:expect} can be done through Monte Carlo sampling.
But for rare-event simulations where the probability of a target event occurring is relatively small, this estimate becomes inadequate.
The challenge of the minimization in \cref{eq:min} then becomes choosing the density function for the true distribution $f(\vec{x})$. 
Importance sampling tells us that the optimal importance sampling density can be reduced to
\begin{align*}
    f^*(\vec{x}) = \frac{\mathbbm{1}\{S(\vec{x}) \ge \gamma\}g(\vec{x}; \vec{\theta})}{\ell}
\end{align*}
thus resulting in the optimization problem:
\begin{align*}
    \vec{\theta}_g^* &= \argmin_{\vec{\theta}_g} - \int\limits_{\vec{x} \in \mathcal{X}} f^*(\vec{x})\log g(\vec{x}; \vec{\theta}_g) d\vec{x}\\
                   &= \argmin_{\vec{\theta}_g} - \int\limits_{\vec{x} \in \mathcal{X}} \frac{\mathbbm{1}\{S(\vec{x}) \ge \gamma\}g(\vec{x}; \vec{\theta})}{\ell}\log g(\vec{x}; \vec{\theta}_g) d\vec{x}
\end{align*}
Note that since we assume $f$ and $g$ belong to the same family of distributions, we get that $f(\vec{x}) = g(\vec{x}; \vec{\theta}_g)$.
Now notice that $\ell$ is independent of $\vec{\theta}_g$, thus we can drop $\ell$ and get the final optimization problem of:
\begin{align} \label{eq:opt}
    \vec{\theta}_g^* &= \argmin_{\vec{\theta}_g} - \int\limits_{\vec{x} \in \mathcal{X}} \mathbbm{1}\{S(\vec{x}) \ge \gamma\}g(\vec{x}; \vec{\theta}) \log g(\vec{x}; \vec{\theta}_g) d\vec{x}\\\nonumber
                   &= \argmin_{\vec{\theta}_g} - \E_{\vec{\theta}}[ \mathbbm{1}\{S(\vec{x}) \ge \gamma\} \log g(\vec{x}; \vec{\theta}_g)]
\end{align}

The CE-method uses a multi-level algorithm to estimate $\vec{\theta}_g^*$ iteratively.
The parameter $\vec{\theta}_k$ at iteration $k$ is used to find new parameters $\vec{\theta}_{k^\prime}$ at the next iteration $k^\prime$.
The threshold $\gamma_k$ becomes smaller that its initial value, thus artificially making events \textit{less rare} under $\vec{X} \sim g(\vec{x}; \vec{\theta}_k)$.

In practice, the CE-method algorithm requires the user to specify a number of \textit{elite} samples $m_\text{elite}$ which are used when fitting the new parameters for iteration $k^\prime$.
Conveniently, if our distribution $g$ belongs to the \textit{natural exponential family} then the optimal parameters can be found analytically \cite{kochenderfer2019algorithms}. For a multivariate Gaussian distribution parameterized by $\vec{\mu}$ and $\mat{\Sigma}$, the optimal parameters for the next iteration $k^\prime$ correspond to the maximum likelihood estimate (MLE):
\begin{align*}
    \vec{\mu}_{k^\prime} &= \frac{1}{m_\text{elite}} \sum_{i=1}^{m_\text{elite}} \vec{x}_i\\
    \vec{\Sigma}_{k^\prime} &= \frac{1}{m_\text{elite}} \sum_{i=1}^{m_\text{elite}} (\vec{x}_i - \vec{\mu}_{k^\prime})(\vec{x}_i - \vec{\mu}_{k^\prime})^\top
\end{align*}

The cross-entropy method algorithm is shown in \cref{alg:cem} \cite{kochenderfer2019algorithms}
For an objective function $S$ and input distribution $g$, the CE-method algorithm will run for $k_\text{max}$ iterations.
At each iteration, $m$ inputs are sampled from $g$ and evaluated using the objective function $S$.
The sampled inputs are denoted by $\mat{X}$ and the evaluated values are denoted by $\mat{Y}$.
Next, the top $m_\text{elite}$ samples are stored in the elite set $\e$, and the distribution $g$ is fit to the elites.
This process is repeated for $k_\text{max}$ iterations and the resulting parameters $\vec{\theta}_{k_\text{max}}$ are returned.
Note that a variety of input distributions for $g$ are supported, but we focus on the multivariate Gaussian distribution and the Gaussian mixture model in this work.

% CE-method
\begin{algorithm}[ht]
  \begin{algorithmic}
  \Function{CrossEntropyMethod}{}($S, g, m, m_\text{elite}, k_\text{max}$)
    \For {$k \in [1,\ldots,k_\text{max}]$}
        \State $\mat{X} \sim g(\;\cdot\;; \vec{\theta}_k)$ where $\mat{X} \in \R^{|g|\times m}$\algorithmiccomment{draw $m$ samples from $g$}
        \State $\mat{Y} \leftarrow S(\vec{x})$ for $\vec{x} \in \mat{X}$ \algorithmiccomment{evaluate samples $\mat{X}$ using objective $S$}
        \State $\e \leftarrow$ store top $m_\text{elite}$ from $\mat{Y}$ \algorithmiccomment{select elite samples output from objective}
        \State $\vec{\theta}_{k^\prime} \leftarrow \textproc{Fit}(g(\;\cdot\;; \vec{\theta}_k), \e)$ \algorithmiccomment{re-fit distribution $g$ using elite samples}
    \EndFor
    \State \Return $g(\;\cdot\;; \vec{\theta}_{k_\text{max}})$
  \EndFunction
  \end{algorithmic}
  \caption{\label{alg:cem} Cross-entropy method.}
\end{algorithm}


\subsection{Mixture Models}
A standard Gaussian distribution is \textit{unimodal} and can have trouble generalizing over data that is \textit{multimodal}.
A \textit{mixture model} is a weighted mixture of unimodal component distributions used to represent continuous multimodal distributions \cite{kochenderfer2015decision}.
Formally, a Gaussian mixture model (GMM) is defined by its parameters $\vec{\mu}$ and $\mat{\Sigma}$ and associated weights $\w$ where $\sum_{i=1}^n w_i = 1$. We denote that a random variable $\mat{X}$ is distributed according to a mixture model as $\mat{X} \sim \operatorname{Mixture}(\vec{\mu}, \vec{\Sigma}, \vec{w})$.
The probability density of the GMM then becomes:
% Mixture model PDF
\begin{gather*}
    P( \mat{X} = \vec{x} ; \vec{\mu}, \mat{\Sigma}, \vec{w}) = \sum_{i=1}^n w_i \Normal(\vec{x} ; \vec{\mu}_i, \mat{\Sigma}_i)
\end{gather*}

To fit the parameters of a Gaussian mixture model, it is well known that the \textit{expectation-maximization} (EM) algorithm can be used \cite{dempster1977maximum,aitkin1980mixture}. 
The EM algorithm seeks to find the maximum likelihood estimate of a dependent latent variable using observed data.
Intuitively, the algorithm alternates between an expectation step (E-step) and a maximization step (M-step) to guarantee convergence to a local minima.
We refer to \cite{dempster1977maximum,aitkin1980mixture} for further reading on the EM algorithm.



\subsection{Surrogate Models}
In the context of optimization, a surrogate model $\hat{S}$ is used to estimate the true objective function and provide less expensive evaluations.
Surrogate models are a popular approach and have been used to evaluate rare-event probabilities in computationally expensive systems \cite{li2010evaluation,li2011efficient}.
The simplest example of a surrogate model is linear regression.
In this work, we focus on the \textit{Gaussian process} surrogate model.
A Gaussian process (GP) is a distribution over functions that predicts the underlying objective function $S$ and captures the uncertainty of the prediction using a probability distribution \cite{kochenderfer2019algorithms}.
This means a GP can be sampled to generate random functions, which can then be fit to our given data $\mat{X}$.
A Gaussian process is parameterized by a mean function $\m(\mat{X})$ and kernel function $\mat{K}(\mat{X},\mat{X})$, which captures the relationship between data points as covariance values.
We denote a Gaussian process that produces estimates $\hat{\vec{y}}$ as:
\begin{align*}
\hat{\vec{y}} &\sim\mathcal{N}\left(\vec{m}(\mat{X}),\vec{K}(\mat{X},\mat{X})\right)\\
        &= \begin{bmatrix} % Changed `m` to `n`
            \hat{S}(\vec{x}_1), \ldots, \hat{S}(\vec{x}_n)
        \end{bmatrix}
\end{align*}
where
\begin{gather*}
\vec{m}(\mat{X}) = \begin{bmatrix} m(\vec{x}_1), \ldots, m(\vec{x}_n) \end{bmatrix}\\
\vec{K}(\mat{X}, \mat{X}) = \begin{bmatrix}
         k(\vec{x}_1, \vec{x}_1) & \cdots & k(\vec{x}_1, \vec{x}_n)\\
         \vdots & \ddots & \vdots\\
         k(\vec{x}_n, \vec{x}_1) & \cdots & k(\vec{x}_n, \vec{x}_n)
     \end{bmatrix}
\end{gather*}
Note that we use the zero-mean function $m(\vec{x}_i) = \vec{0}$, which is generally conventional.
For the kernel function $k(\vec{x}_i, \vec{x}_i)$, we use the squared exponential kernel with variance $\sigma^2$ and characteristic scale-length $\ell$, where larger $\ell$ values increase the correlation between successive data points, thus smoothing out the generated functions. The squared exponential kernel is defined as:
% Isotropic Squared Exponential kernel (covariance): \exp(-\frac{r^2}{2\ell^2})
\begin{align*}
k(\vec{x},\vec{x}^\prime) = \sigma^2\exp\left(- \frac{(\vec{x} - \vec{x}^\prime)^\top(\vec{x} - \vec{x}^\prime)}{2\ell^2}\right)
\end{align*}
We refer to \cite{kochenderfer2019algorithms} for a detailed overview of Gaussian processes and different kernel functions.



\section{Algorithms} \label{sec:cem_algorithms}
We can now describe the cross-entropy method variants introduced in this work.\footnote{Code available at \href{https://github.com/mossr/CrossEntropyVariants.jl}{https://github.com/mossr/CrossEntropyVariants.jl}}
This section will first cover the main algorithm introduced, the \textit{cross-entropy surrogate method} (CE-surrogate).
Then we introduce a modification to the CE-surrogate method, namely the \textit{cross-entropy mixture method} (CE-mixture).
Lastly, we describe various evaluation schedules for redistributing objective function calls over the iterations.

\subsection{Cross-Entropy Surrogate Method} \label{sec:cem_alg_ce_surrogate}
The main CE-method variant we introduce is the \textit{cross-entropy surrogate method} (CE-surrogate).
The CE-surrogate method is a superset of the CE-method, where the differences lie in the evaluation scheduling and modeling of the elite set using a surrogate model.
The goal of the CE-surrogate algorithm is to address the shortcomings of the CE-method when the number of objective function calls is sparse and the underlying objective function $S$ has multiple local minima.

The CE-surrogate algorithm is shown in \cref{alg:ce_surrogate}.
It takes as input the objective function $S$, the distribution $\M$ parameterized by $\vec{\theta}$, the number of samples $m$, the number of elite samples $m_\text{elite}$, and the maximum iterations $k_\text{max}$.
For each iteration $k$, the number of samples $m$ are redistributed through a call to \smallcaps{EvaluationSchedule}, where $m$ controls the number of true objective function evaluations of $S$. % EvaluationSchedule.
Then, the algorithm samples from $\M$ parameterized by the current $\vec{\theta}_k$ given the adjusted number of samples $m$.
For each sample in $\mat{X}$, the objective function $S$ is evaluated and the results are then stored in $\mat{Y}$.
The top $m_\text{elite}$ evaluations from $\mat{Y}$ are stored in $\e$. 
Using all of the current function evaluations $\mat{Y}$ from sampled inputs $\mat{X}$, a modeled elite set $\bfE$ is created to augment the sparse information provided by a low number of true objective function evaluations.
Finally, the distribution $\M$ is fit to the elite set $\bfE$ and the distribution with the final parameters $\vec{\theta}_{k_\text{max}}$ is returned.

\vspace{5mm} % NOTE. To complement the \newpage below.

% CE-surrogate
\begin{algorithm}[ht]
  \begin{algorithmic}
  \Function{CE-Surrogate}{$S$, $\M$, $m$, $m_\text{elite}$, $k_\text{max}$}
    \For {$k \in [1,\ldots,k_\text{max}]$}
        \State $m, m_\text{elite} \leftarrow \textproc{EvaluationSchedule}(k, k_\text{max})$ \algorithmiccomment{number of evaluations from a schedule}
        \State $\mat{X} \sim \M(\;\cdot\;; \vec{\theta}_k)$ where $\mat{X} \in \R^{|\M| \times m}$ \algorithmiccomment{draw $m$ samples from $\M$}
        \State $\mat{Y} \leftarrow S(\vec{x})$ for $\vec{x} \in \mat{X}$ \algorithmiccomment{evaluate samples $\mat{X}$ using true objective $S$}
        \State $\e \leftarrow$ store top $m_\text{elite}$ from $\mat{Y}$ \algorithmiccomment{select elite samples output from true objective}
        \State $\bfE \leftarrow \textproc{ModelEliteSet}(\mat{X}, \mat{Y}, \M, \e, m, m_\text{elite})$ \algorithmiccomment{find model-elites using a surrogate model}
        \State $\vec{\theta}_{k^\prime} \leftarrow \textproc{Fit}(\M(\;\cdot\;; \vec{\theta}_k), \bfE)$ \algorithmiccomment{re-fit distribution $\M$ using model-elite samples}
    \EndFor
    \State \Return $\M(\;\cdot\;; \vec{\theta}_{k_\text{max}})$
  \EndFunction
  \end{algorithmic}
  \caption{\label{alg:ce_surrogate} Cross-entropy surrogate method.}
\end{algorithm}

\newpage % NOTE.

The main difference between the standard CE-method and the CE-surrogate variant lies in the call to \cref{alg:model_elite_set}, \smallcaps{ModelEliteSet}.
The motivation is to use \textit{all} of the already evaluated objective function values $\mat{Y}$ from a set of sampled inputs $\mat{X}$.
This way, the expensive function evaluations---otherwise discarded---can be used to build a surrogate model of the underlying objective function.
First, a surrogate model $\surrogate$ is constructed from the samples $\mat{X}$ and true objective function values $\mat{Y}$.
We used a Gaussian process with a specified kernel and optimizer, but other surrogate modeling techniques such as regression with basis functions can be used.
We chose a Gaussian process because it incorporates probabilistic uncertainty in the predictions, which may more accurately represent our objective function, or at least be sensitive to over-fitting to sparse data.
Now we have an approximated objective function $\surrogate$ that we can inexpensively call. 
We sample $10m$ values from the distribution $\M$ and evaluate them using the surrogate model.
We then store the top $10m_\text{elite}$ values from the estimates $\mathbf{\hat{\mat{Y}}}_\text{m}$.
We call these estimated elite values $\e_\text{model}$ the \textit{model-elites}.
The surrogate model is then passed to \smallcaps{SubEliteSet}, which returns more estimates for elite values.
Finally, the elite set $\bfE$ is built from the true-elites $\e$, the model-elites $\e_\text{model}$, and the subcomponent-elites $\e_\text{sub}$.
The resulting concatenated elite set $\bfE$ is returned.

\begin{algorithm}[ht]
  \begin{algorithmic}
  \Function{ModelEliteSet}{$\mat{X}, \mat{Y}, \M, \e, m, m_\text{elite}$}
    % Fit to entire population!
    \State $\surrogate \leftarrow \textproc{GaussianProcess}(\mat{X}, \mat{Y}, \text{kernel}, \text{optimizer})$ \algorithmiccomment{fit a surrogate model to the samples} % Squared exponential, NelderMead
    \State $\mat{X}_\text{m} \sim \M(\;\cdot\;; \vec{\theta}_k)$ where $\mat{X}_\text{m} \in \R^{|\M| \times {10m}}$ \algorithmiccomment{draw $10m$ samples from $\M$}
    \State $\mathbf{\hat{\mat{Y}}}_\text{m} \leftarrow \surrogate(\vec{x}_\text{m})$ for $\vec{x}_\text{m} \in \mat{X}_\text{m}$ \algorithmiccomment{evaluate samples $\mat{X}_\text{m}$ using surrogate objective $\surrogate$}
    \State $\e_\text{model} \leftarrow$ store top $10m_\text{elite}$ from $\mathbf{\hat{\mat{Y}}}_\text{m}$ \algorithmiccomment{select model-elite samples from surrogate objective}
    \State $\e_\text{sub} \leftarrow \textproc{SubEliteSet}(\surrogate, \M, \e)$ \algorithmiccomment{generate sub-elite samples using surrogate $\surrogate$}
    \State $\bfE \leftarrow \{ \e \} \cup \{ \e_\text{model} \} \cup \{ \e_\text{sub} \}$ \algorithmiccomment{combine all elite samples into an elite set}
    \State \Return $\bfE$
  \EndFunction
  \end{algorithmic}
  \caption{\label{alg:model_elite_set} Modeling elite set using a surrogate objective.}
\end{algorithm}

To encourage exploration of promising areas of the design space, the algorithm \smallcaps{SubEliteSet} focuses on the already marked true-elites $\e$.
Each elite $e_x \in \e$ is used as the mean of a new multivariate Gaussian distribution with covariance inherited from the distribution $\M$.
The collection of subcomponent distributions is stored in $\m$.
The idea is to use the information given to us by the true-elites to emphasize areas of the design space that look promising.
For each distribution $\m_i \in \m$, we run a subroutine call to the standard CE-method to fit the distribution $\m_i$ using the surrogate model $\surrogate$. 
Then the best objective function value is added to the subcomponent-elite set $\e_\text{sub}$, and after iterating the full set is returned.
Note that we use $\theta_\text{CE}$ to denote the parameters for the CE-method algorithm.
In our case, we recommend using a small $k_\text{max}$ of around $2$ so the subcomponent-elites do not over-fit to the surrogate model but have enough CE-method iterations to tend towards optimal.

\begin{algorithm}[ht]
  \begin{algorithmic}
  \Function{SubEliteSet}{$\surrogate, \M, \e$}
    \State $\e_\text{sub} \leftarrow \emptyset$
    \State $\m \leftarrow \{ e_x \in \e \mid \Normal(e_x, \M.\Sigma) \}$ \algorithmiccomment{create set of distributions centered at each true-elite sample}
    \For {$\m_i \in \m$}
        \State $\m_i \leftarrow \textproc{CrossEntropyMethod}(\surrogate, \m_i ; \theta_{\text{CE}})$ \algorithmiccomment{run CE-method over each new distribution}
        \State $\e_\text{sub} \leftarrow \{\e_\text{sub}\} \cup \{\textproc{Best}(\m_i)\}$ \algorithmiccomment{append best result into the sub-elite set}
    \EndFor
    \State \Return $\e_\text{sub}$
  \EndFunction
  \end{algorithmic}
  \caption{\label{alg:sub_elite_set} Subcomponent elite set.}
\end{algorithm}


\subsection{Cross-Entropy Mixture Method} \label{sec:cem_alg_ce_mixture}
We refer to the variant of our CE-surrogate method that takes an input mixture model $\M$ as the \textit{cross-entropy mixture method} (CE-mixture).
The CE-mixture algorithm is identical to the CE-surrogate algorithm, but calls a custom \smallcaps{Fit} function to fit a mixture model to the elite set $\bfE$.
The input distribution $\M$ is cast to a mixture model using the subcomponent distributions $\m$ as the components of the mixture.
We use a default uniform weighting for each mixture component.
The mixture model $\M$ is then fit using the expectation-maximization algorithm, and the resulting distribution is returned.
The idea is to use the distributions in $\m$ that are centered around each true-elite as the components of the casted mixture model.
Therefore, we would expect better performance of the CE-mixture method when the objective function has many competing local minima.
Results in \cref{sec:cem_results} aim to show this behavior.

% CE-mixture (fit)
\begin{algorithm}[ht]
  \begin{algorithmic}
  \Function{Fit}{$\M, \m, \bfE$}
    \State $\M \leftarrow \operatorname{Mixture}( \m )$
    \State $\mathbf{\hat{\vec{\theta}}} \leftarrow \textproc{ExpectationMaximization}(\M, \bfE)$
    \State \Return $\M(\;\cdot\;; \mathbf{\hat{\vec{\theta}}})$
  \EndFunction
  \end{algorithmic}
  \caption{\label{alg:ce_mixture_fit} Fitting mixture models (used by CE-mixture).}
\end{algorithm}


\subsection{Evaluation Scheduling} \label{sec:cem_alg_eval_schedule}
Given the nature of the CE-method, we expect the covariance to shrink over time, thus resulting in a solution with higher confidence.
Yet if each iteration is given the same number of objective function evaluations $m$, there is the potential for elite samples from early iterations dominating the convergence.
Therefore, we would like to redistribute the objective function evaluations throughout the iterations to use more truth information early in the process.
We call these heuristics \textit{evaluation schedules}.
One way to achieve this is to reallocate the evaluations according to a Geometric distribution.
Evaluation schedules can also be ad-hoc and manually prescribed based on the current iteration.

We provide the evaluation schedule we use that follows a Geometric distribution with parameter $p$ in \cref{alg:evaluation_schedule}.
We denote $G \sim \Geo(p)$ to be a random variable that follows a truncated Geometric distribution with the probability mass function $p_G(k) = p(1 - p)^k$ for $k \in \{0, 1, 2, \ldots, k_\text{max}\}$. % Geo(p) PMF
Note the use of the integer rounding function (e.g., $\round{x}$), which we later have to compensate for the final iterations.
Results in \cref{sec:cem_results} compare values of $p$ that control the redistribution of evaluations.


% EvaluationSchedule
\begin{algorithm}[ht]
  \begin{algorithmic}
  \Function{EvaluationSchedule}{$k, k_\text{max}$}
    \State $G \sim \Geo(p)$
    \State $N_\text{max} \leftarrow k_\text{max} \cdot m$
    \State $m \leftarrow \round{N_\text{max} \cdot p_G(k)}$
    \If{$k = k_\text{max}$}
        \State $s \leftarrow \displaystyle\sum_{i=1}^{k_\text{max}-1} \round{N_\text{max} \cdot p_G(i)}$
        \State $m \leftarrow \min(N_\text{max} - s, N_\text{max} - m)$
    \EndIf
    \State $m_\text{elite} \leftarrow \min(m_\text{elite}, m)$
    \State \Return ($m, m_\text{elite}$) 
  \EndFunction
  \end{algorithmic}
  \caption{\label{alg:evaluation_schedule} Evaluation schedule using a Geometric distribution.}
\end{algorithm}


\section{Experiments} \label{sec:cem_experiments}
In this section, we detail the experiments we ran to compare the CE-method variants and evaluation schedules.
We first introduce a test objective function we created to stress the issue of converging to local minima. 
We then describe the experimental setup for each of our experiments and provide an analysis and results.


\subsection{Test Objective Function Generation}\label{sec:sierra}
\begin{figure*}[!t]
  \centering
  \resizebox{0.8\textwidth}{!}{\input{figures/cem_variants/sierra_group.tex}}
  \caption{
    \label{fig:sierra}
    Example test objective functions generated using the sierra function.  
  }
\end{figure*}
To stress the cross-entropy method and its variants, we created a test objective function called \textit{sierra} that is generated from a mixture model comprised of $49$ multivariate Gaussian distributions.
We chose this construction so that we can use the negative peeks of the component distributions as local minima and can force a global minimum centered at our desired $\mathbf{\widetilde{\vec{\mu}}}$.
The construction of the sierra test function can be controlled by parameters that define the spread of the local minima.
We first start with the center defined by a mean vector $\mathbf{\widetilde{\vec{\mu}}}$ and we use a common covariance $\mathbf{\widetilde{\mat{\Sigma}}}$:
\begin{align*}
    \mathbf{\widetilde{\vec{\mu}}} &= [\mu_1, \mu_2], \quad \mathbf{\widetilde{\mat{\Sigma}}} = \begin{bmatrix}\sigma & 0\\ 0 & \sigma \end{bmatrix}
\end{align*}
Next, we use the parameter $\delta$ to control the clustered distance between symmetric points:
\begin{align*}
    \mat{G} &= \left\{[+\delta, +\delta], [+\delta, -\delta], [-\delta, +\delta], [-\delta, -\delta]\right\}
\end{align*}
We chose points $\mat{P}$ to fan out the clustered minima relative to the center defined by $\mathbf{\widetilde{\vec{\mu}}}$:
\begin{align*}
    \mat{P} &= \left\{[0, 0], [1, 1], [2, 0], [3, 1], [0, 2], [1, 3]\right\}
\end{align*}
The vector $\vec{s}$ is used to control the $\pm$ distance to create an `s' shape comprised of minima, using the standard deviation $\sigma$:
$\vec{s} = [+\sigma, -\sigma]$.
We set the following default parameters: standard deviation $\sigma=3$, spread rate $\eta=6$, and cluster distance $\delta=2$.
We can also control if the local minima clusters ``decay'', thus making those local minima less distinct (where $\text{decay} \in \{0, 1\})$.
The parameters that define the sierra function are collected into $\vec{\theta} = \langle \mathbf{\widetilde{\vec{\mu}}}, \mathbf{\widetilde{\mat{\Sigma}}}, \mat{G}, \mat{P}, \vec{s} \rangle$.
Using these parameters, we can define the mixture model used by the sierra function as:
\begin{gather*}
    \Sierra \sim \operatorname{Mixture}\left(\left\{ \vec{\theta} ~\Big|~ \Normal\left(\vec{g} +  s\vec{p}_i + \mathbf{\widetilde{\vec{\mu}}},\; \mathbf{\widetilde{\mat{\Sigma}}} \cdot i^{\text{decay}}/\eta \right) \right\} \right) \quad\text{for}\quad (\vec{g}, \vec{p}_i, s) \in (\mat{G}, \mat{P}, \vec{s})
\end{gather*}
We add a final component to be our global minimum centered at $\mathbf{\widetilde{\vec{\mu}}}$ and with a covariance scaled by $\sigma\eta$. Namely, the global minimum is $\vec{x}^* = \E\left[\Normal(\mathbf{\widetilde{\vec{\mu}}}, \mathbf{\widetilde{\mat{\Sigma}}}/(\sigma\eta))\right] = \mathbf{\widetilde{\vec{\mu}}}$.
We can now use this constant mixture model with $49$ components and define the sierra objective function $\mathcal{S}(\vec{x})$ to be the negative probability density of the mixture at input $\vec{x}$ with uniform weights:

\begin{align*}
    \mathcal{S}(\vec{x}) &= -P(\Sierra = \vec{x}) = -\frac{1}{|\Sierra|}\sum_{j=1}^{n}\Normal(\vec{x} ; \vec{\mu}_j, \mat{\Sigma}_j)
\end{align*}
An example of six different objective functions generated using the sierra function are shown in \cref{fig:sierra}, sweeping over the spread rate $\eta$, with and without decay.

\subsection{Experimental Setup} \label{sec:cem_experiment_setup}
Experiments were run to stress a variety of behaviors of each CE-method variant.
The experiments are split into two categories: algorithmic and scheduling.
The algorithmic category aims to compare features of each CE-method variant while holding common parameters constant (for a better comparison).
While the scheduling category experiments with evaluation scheduling heuristics.


Because the algorithms are stochastic, we run each experiment with 50 different random number generator seed values.
To evaluate the performance of the algorithms in their respective experiments, we define three metrics.
First, we define the average ``optimal'' value $\bar{b}_v$ to be the average of the best so-far objective function value (termed ``optimal'' in the context of each algorithm). Again, we emphasize that we average over the 50 seed values to gather meaningful statistics.
Another metric we monitor is the average distance to the true global optimal $\bar{b}_d = \norm{\vec{b}_{\vec{x}} - \vec{x}^*}$, where $\vec{b}_{\vec{x}}$ denotes the $\vec{x}$-value associated with the ``optimal''.
We make the distinction between these metrics to show both ``closeness'' in \textit{value} to the global minimum and ``closeness'' in the \textit{design space} to the global minimum.
Our final metric looks at the average runtime of each algorithm, noting that our goal is to off-load computationally expensive objective function calls to the surrogate model.

For all of the experiments, we use a common setting of the following parameters for the sierra test function (shown in the top-right plot in \cref{fig:sierra}):
\begin{equation*}
    (\mathbf{\widetilde{\vec{\mu}}} =[0,0],\; \sigma=3,\; \delta=2,\; \eta=6,\; \text{decay} = 1)
\end{equation*}

\begin{figure*}[!b]
  \centering
    \subfloat[The cross-entropy method.]{%
    \resizebox{0.3\textwidth}{!}{\input{figures/cem_variants/k5_ce_method.pgf}}
    }
    \subfloat[The cross-entropy surrogate method.]{%
    \resizebox{0.3\textwidth}{!}{\input{figures/cem_variants/k5_ce_surrogate.pgf}}
    }
    \subfloat[The cross-entropy mixture method.]{%
    \resizebox{0.3\textwidth}{!}{\input{figures/cem_variants/k5_ce_mixture.pgf}}
    }
  \caption{
    \label{fig:k5} Iteration $k=5$ illustrated for each algorithm. The covariance is shown by the contours.
  } 
\end{figure*}


\subsubsection{Algorithmic Experiments} \label{sec:cem_alg_experiments}
We run three separate algorithmic experiments, each to test a specific feature.
For our first algorithmic experiment (1A), we want to test each algorithm when the user-defined mean is centered at the global minimum and the covariance is arbitrarily wide enough to cover the design space.
\Cref{fig:k5} illustrates experiment (1A) for each algorithm.
Let $\M$ be a distribution parameterized by $\vec{\theta} = (\vec{\mu}, \mat{\Sigma})$, and for experiment (1A) we set the following:
% CE-mixture mean and covariance (1A)
\begin{equation*}
    \vec\mu^{(\text{1A})} = [0, 0] \qquad
    \mat\Sigma^{(\text{1A})} = \begin{bmatrix}
        200 & 0\\
        0 & 200
    \end{bmatrix}
\end{equation*}

For our second algorithmic experiment (1B), we test a mean that is far off-centered with a wider covariance:
% CE-mixture mean and covariance (1B)
\begin{equation*}
    \vec\mu^{(\text{1B})} = [-50, -50] \qquad
    \mat\Sigma^{(\text{1B})} = \begin{bmatrix}
        2000 & 0\\
        0 & 2000
    \end{bmatrix}
\end{equation*}
This experiment is used to test the ``exploration'' of the CE-method variants introduced in this work.
In experiments (1A) and (1B), we set the following common parameters across each CE-method variant:
% CE-mixture hyperparameter settings
\begin{equation*}
    (k_\text{max} = 10,\; m=10,\; m_\text{elite}=5)^{(\text{1A,1B})}
\end{equation*}
This results in $m\cdot k_\text{max} = 100$ objective function evaluations, which we define to be \textit{relatively} low.

For our third algorithmic experiment (1C), we want to test how each variant responds to an extremely low number of function evaluations.
This sparse experiment sets the common CE-method parameters to:
% CE-method params (1C)
\begin{equation*}
    (k_\text{max} = 10,\; m=5,\; m_\text{elite}=3)^{(\text{1C})}
\end{equation*}
This results in $m\cdot k_\text{max} = 50$ objective function evaluations, which we defined to be \textit{extremely} low.
We use the same mean and covariance defined for experiment (1A):
\begin{equation*}
    \vec\mu^{(\text{1C})} = [0, 0] \qquad
    \mat\Sigma^{(\text{1C})} = \begin{bmatrix}
        200 & 0\\
        0 & 200
    \end{bmatrix}
\end{equation*}


\vspace{-8mm} % NOTE. Fix orphans.
\subsubsection{Scheduling Experiments} \label{sec:cem_schedule_experiments}
In our final experiment (2), we test the evaluation scheduling heuristics which are based on the Geometric distribution.
We sweep over the parameter $p$ that determines the Geometric distribution which controls the redistribution of objective function evaluations.
In this experiment, we compare the CE-surrogate methods using the same setup as experiment (1B), namely the far off-centered mean.
We chose this setup to analyze exploration schemes when given very little information about the true objective function.



\subsection{Results and Analysis} \label{sec:cem_results}

\Cref{fig:experiment_1a} shows the average value of the current optimal $\bar{b}_v$ for the three algorithms for experiment (1A). 
One standard deviation is plotted in the shaded region.
Notice that the standard CE-method converges to a local minima before $k_\text{max}$ is reached.
Both CE-surrogate method and CE-mixture stay below the standard CE-method curve, highlighting the mitigation of convergence to local minima.
Minor differences can be seen between CE-surrogate and CE-mixture, differing slightly towards the tail in favor of CE-surrogate.
The average runtime of the algorithms along with the performance metrics are shown together for each experiment in \cref{tab:cem_results}.

\begin{figure*}[ht]
    \centering
    \subfloat[Average optimal value for experiment (1A) when the initial mean is centered at the global minimum and the covariance sufficiently covers the design space.]{%
        \resizebox{0.47\textwidth}{!}{\input{figures/cem_variants/experiment1a.tex}}
        \label{fig:experiment_1a}
    }
    \hspace{2mm}
    \subfloat[Average optimal value for experiment (1B) when the initial mean is far from the global minimum with a wide covariance.]{%
        \resizebox{0.47\textwidth}{!}{\input{figures/cem_variants/experiment1b.tex}}
        \label{fig:experiment_1b}
    }
    \hspace{2mm}
    \subfloat[Average optimal value for experiment (1C) when we restrict the number of objective function calls.]{%
        \resizebox{0.47\textwidth}{!}{\input{figures/cem_variants/experiment1c.tex}}
        \label{fig:experiment_1c}
    }
    \caption{Cross-entropy method variant experiment results.}\label{fig:cem_experiments}
\end{figure*}


An apparent benefit of the standard CE-method is in its simplicity and speed.
As shown in \cref{tab:cem_results}, the CE-method is the fastest approach by about 2-3 orders of magnitude compared to CE-surrogate and CE-mixture.
The CE-mixture method is notably the slowest approach.
Although the runtime is also based on the objective function being tested, recall that we are using the same number of true objective function calls in each algorithm, and the metrics we are concerned with in optimization are to minimize $\bar{b}_v$ and $\bar{b}_d$.
We can see that the CE-surrogate method consistently out performs the other methods.
Surprisingly, a uniform evaluation schedule performs the best even in the sparse scenario where the initial mean is far away from the global optimal.


When the initial mean of the input distribution is placed far away from the global optimal, the CE-method tends to converge prematurely as shown in \cref{fig:experiment_1b}.
This scenario is illustrated in \cref{fig:example_1b}.
We can see that both CE-surrogate and CE-mixture perform well in this case.


Given the same centered mean as before, when we restrict the number of objective function calls even further to just 50 we see interesting behavior.
Notice that the results of experiment (1C) shown in \cref{fig:experiment_1c} follow a curve closer to the far away mean from experiment (1B) than from the same setup as experiment (1A). Also notice that the CE-surrogate results cap out at iteration 9 due to the evaluation schedule front-loading the objective function calls, thus leaving none for the final iteration (while still maintaining the same total number of evaluations of 50).

\begin{table}[!ht]
    \small
    \centering
    \caption{\label{tab:cem_results} Experimental Results.}
    \begin{tabular}{cllll} % p{3cm}
    \toprule
    \textbf{Exper.} & \textbf{Algorithm} & \textbf{Runtime} & $\bar{b}_v$ & $\bar{b}_d$\\
    \midrule
    \multirow{3}{*}{1A} & CE-method & \textbf{0.029 $\operatorname{s}$} & $-$0.0134 & 23.48\\
    &CE-surrogate & 1.47 $\operatorname{s}$ & \textbf{\boldmath$-$0.0179} & \textbf{12.23}\\
    &CE-mixture & 9.17 $\operatorname{s}$ & $-$0.0169 & 16.87\\
    \midrule
    \multirow{3}{*}{1B} & CE-method & \textbf{0.046 $\operatorname{s}$} & $-$0.0032 & 138.87\\
    &CE-surrogate & 11.82 $\operatorname{s}$ & \textbf{\boldmath$-$0.0156} & \textbf{18.24}\\
    &CE-mixture & 28.10 $\operatorname{s}$ & $-$0.0146 & 33.30\\
    \midrule
    \multirow{3}{*}{1C} & CE-method & \textbf{0.052 $\operatorname{s}$} & $-$0.0065 & 43.14\\
    &CE-surrogate & 0.474 $\operatorname{s}$ & \textbf{\boldmath$-$0.0156} & \textbf{17.23}\\
    &CE-mixture & 2.57 $\operatorname{s}$ & $-$0.0146 & 22.17\\
    \midrule
    \multirow{3}{*}{2} & CE-surrogate, $\operatorname{Uniform}$ & --- & \textbf{\boldmath$-$0.0193} & \textbf{8.53}\\
    &CE-surrogate, $\Geo(0.1)$ & {\color{gray}---} & $-$0.0115 & 25.35\\
    &CE-surrogate, $\Geo(0.2)$ & {\color{gray}---} & $-$0.0099 & 27.59\\
    &CE-surrogate, $\Geo(0.3)$ & {\color{gray}---} & $-$0.0089 & 30.88\\
    \bottomrule
    & & & \multicolumn{2}{l}{$-\text{0.0220} \approx\vec{x}^*$}\\
    \end{tabular}
\end{table}


\section{Discussion} \label{sec:cem_discussion}
We presented variants of the popular cross-entropy method for optimization of objective functions with multiple local minima.
Using a Gaussian process-based surrogate model, we can use the same number of true objective function evaluations and achieve better performance than the standard CE-method on average.
We also explored the use of a Gaussian mixture model to help find global minimum in multimodal objective functions.
We introduce a parameterized test objective function with a controllable global minimum and spread of local minima.
Using this test function, we showed that the CE-surrogate algorithm achieves the best performance relative to the standard CE-method, each using the same number of true objective function evaluations.


\begin{figure}[!h]
  \centering
  \resizebox{0.6\columnwidth}{!}{\input{figures/cem_variants/example1b.pgf}}
  \caption{
    \label{fig:example_1b}
    First iteration of the scenario in experiment (1B) where the initial distribution is far away form the global optimal. The red dots indicate the true-elites, the magenta dots with black outlines indicate the ``non-elites'' (i.e., all others) evaluated from the true objective function, and the white dots with black outlines indicate the samples evaluated using the surrogate model. Notice the trend of the white dots (those from the surrogate) that can help guide the proposal distribution towards the area of interest (in the top right).
  }
\end{figure}
